Yesterday the team created a sample prototype video to model the whole interaction. The prototype successfuly depicts the way the globe rotates, speech based commanding, navigation and the interaction with the user. Due to shortage of time the protoype didn't cover the display of cultural and racial content of the area that the user navigated. We plan to use google search API and wikipedia API to enlighten the user with content related to their search.
http://youtu.be/e2ldhG_skFg
And few images captured in the process are down here:
![Prototype diagrams](../project_images/prototype1.jpg)
![Prototype diagrams](../project_images/prototype2.jpg)
![Prototype diagrams](../project_images/prototype3.jpg)
![Prototype diagrams](../project_images/prototype4.jpg)
![Prototype diagrams](../project_images/prototype5.jpg)
![Prototype diagrams](../project_images/prototype6.jpg)


